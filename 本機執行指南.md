# 104職缺爬蟲 - 本機執行版本

不需要Docker,直接在Python環境執行!

## 環境需求
- Python 3.8 或以上
- pip 套件管理工具

## 安裝步驟

### 1. 安裝必要套件
```bash
pip install scrapy
```

就這麼簡單!只需要安裝 Scrapy 即可。

### 2. 準備檔案結構
```
your_project/
├── scrapy.cfg
├── scraper/
│   ├── __init__.py
│   ├── settings.py        (使用 settings_simple.py 的內容)
│   ├── pipelines.py       (使用 pipelines_simple.py 的內容)
│   └── spiders/
│       ├── __init__.py
│       └── a104.py        (使用 a104_simple.py 的內容)
```

### 3. 執行爬蟲

在專案根目錄執行:

```bash
scrapy crawl 104_ai_jobs
```

或指定輸出檔名:

```bash
scrapy crawl 104_ai_jobs -o ai_jobs_output.csv
```

## 執行結果

爬蟲會:
- 搜尋6個AI相關關鍵字(AI自動化、AI轉型、數位轉型、流程自動化、RPA、AI工程師)
- 每個關鍵字爬取5頁
- 自動去除重複職缺
- 輸出為CSV檔案(支援中文)

預計會產生 **ai_jobs_YYYYMMDD_HHMMSS.csv** 檔案。

## 可調整的參數

在 `a104_simple.py` 中:

```python
# 修改搜尋關鍵字
keywords = [
    "AI自動化",
    "AI轉型", 
    "數位轉型",
    # 可以加入更多關鍵字...
]

# 修改每個關鍵字爬取的頁數
pages_per_keyword = 5  # 可改為 1-10
```

在 `settings_simple.py` 中:

```python
# 修改請求延遲(秒)
DOWNLOAD_DELAY = 0.5  # 避免被104封鎖,建議0.5-2秒

# 修改同時請求數
CONCURRENT_REQUESTS = 16  # 建議8-16
```

## 常見問題

### Q: 爬不到資料?
A: 檢查網路連線,或增加 `DOWNLOAD_DELAY` 延遲時間

### Q: 被104封鎖IP?
A: 增加延遲時間,減少 `CONCURRENT_REQUESTS`,或過一段時間再試

### Q: CSV亂碼?
A: 用Excel開啟時選擇「UTF-8」編碼

### Q: 想要更多資料?
A: 增加 `pages_per_keyword` 或加入更多關鍵字

## 輸出欄位說明

- search_keyword: 搜尋時使用的關鍵字
- jobName: 職缺名稱
- custName: 公司名稱
- description: 工作內容描述
- salaryLow/salaryHigh: 薪資範圍
- jobAddress: 工作地點
- jobLink: 職缺連結
- remoteWorkType: 遠端工作類型(0=不可 1=完全 2=部分)
- 更多欄位請見CSV...

## 注意事項

⚠️ **使用限制**
- 僅供個人研究學習使用
- 請勿用於商業用途
- 請勿過度頻繁爬取
- 尊重104網站服務條款

⚠️ **法律提醒**
- 爬取公開資訊但仍需合理使用
- 建議設定適當延遲避免造成伺服器負擔
- 大量商業使用請洽詢104官方API服務
