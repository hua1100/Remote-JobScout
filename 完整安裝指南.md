# 104 AI職缺爬蟲 - 完整安裝指南

## 📋 目錄
1. [環境需求](#環境需求)
2. [快速開始](#快速開始)
3. [詳細安裝步驟](#詳細安裝步驟)
4. [設定說明](#設定說明)
5. [執行爬蟲](#執行爬蟲)
6. [查看結果](#查看結果)
7. [進階設定](#進階設定)
8. [常見問題](#常見問題)

---

## 環境需求

- **Python 3.8+** (建議使用 3.11 或 3.12)
- **pip** (Python套件管理工具)
- **網路連線**

---

## 快速開始 (5分鐘)

```bash
# 1. 安裝套件
pip install -r requirements.txt

# 2. 確認.env設定檔存在 (已提供預設值)
# 可直接使用,或依需求修改

# 3. 執行爬蟲
cd 你的專案目錄
scrapy crawl 104_ai_jobs

# 完成! 會產生 ai_jobs_20250101_120000.csv 檔案
```

---

## 詳細安裝步驟

### Step 1: 檢查Python版本

```bash
python --version
# 或
python3 --version

# 應該顯示 Python 3.8.x 或更高版本
```

如果沒有Python,請到 [python.org](https://www.python.org/downloads/) 下載安裝。

### Step 2: 下載專案檔案

確保你有以下檔案結構:

```
your_project/
├── .env                    # 設定檔 ✅ 已建立
├── requirements.txt        # 套件清單 ✅ 已建立
├── scrapy.cfg             # Scrapy配置
├── scraper/
│   ├── __init__.py
│   ├── settings.py        # 使用 settings_with_env.py
│   ├── pipelines.py       # 使用 pipelines_simple.py
│   └── spiders/
│       ├── __init__.py
│       └── a104.py        # 使用 a104_with_env.py
```

### Step 3: 安裝Python套件

```bash
# 在專案目錄下執行
pip install -r requirements.txt

# 或使用 pip3
pip3 install -r requirements.txt
```

安裝完成後會顯示:
```
Successfully installed scrapy-2.11.0 python-dotenv-1.0.0 ...
```

### Step 4: 驗證安裝

```bash
scrapy version
# 應該顯示: Scrapy 2.11.0 或更高版本
```

---

## 設定說明

### .env 設定檔

所有設定都在 `.env` 檔案中,已提供預設值:

```bash
# 核心設定 (可依需求調整)
SCRAPY_PAGES_PER_KEYWORD=5        # 每個關鍵字爬幾頁 (建議1-10)
DOWNLOAD_DELAY=0.5                 # 請求延遲(秒),避免被封鎖
CONCURRENT_REQUESTS=16             # 同時發送請求數

# 搜尋關鍵字 (用逗號分隔)
SEARCH_KEYWORDS="AI自動化,AI轉型,數位轉型,流程自動化,RPA,AI工程師"

# 輸出設定
OUTPUT_FILENAME="ai_jobs"          # 輸出檔案名稱
OUTPUT_FORMAT="csv"                # 輸出格式
```

### 自訂搜尋關鍵字

編輯 `.env` 檔案中的 `SEARCH_KEYWORDS`:

```bash
# 範例1: 只搜尋特定關鍵字
SEARCH_KEYWORDS="AI自動化,RPA"

# 範例2: 加入更多關鍵字
SEARCH_KEYWORDS="AI自動化,AI轉型,數位轉型,流程自動化,RPA,AI工程師,機器學習,深度學習"

# 範例3: 搜尋特定職位
SEARCH_KEYWORDS="資料科學家,數據分析師,ML Engineer"
```

---

## 執行爬蟲

### 基本執行

```bash
# 在專案根目錄執行
scrapy crawl 104_ai_jobs
```

### 指定輸出檔名

```bash
scrapy crawl 104_ai_jobs -o my_jobs.csv
```

### 執行過程會看到

```
2025-01-30 12:00:00 [scrapy.core.engine] INFO: Spider opened
搜尋關鍵字: AI自動化, AI轉型, 數位轉型, 流程自動化, RPA, AI工程師
每個關鍵字爬取: 5 頁
關鍵字 "AI自動化" 第1頁: 找到 20 筆職缺
關鍵字 "AI自動化" 第2頁: 找到 20 筆職缺
...
2025-01-30 12:05:30 [scrapy.core.engine] INFO: Spider closed (finished)
```

### 執行時間估算

- 1個關鍵字 × 5頁 ≈ 1-2分鐘
- 6個關鍵字 × 5頁 ≈ 5-10分鐘

---

## 查看結果

### CSV檔案位置

執行完成後,會在專案目錄產生:

```
ai_jobs_20250130_120530.csv
```

### 用Excel開啟

1. 雙擊CSV檔案
2. 如果中文顯示正常,完成!
3. 如果出現亂碼:
   - Excel → 資料 → 從文字檔
   - 選擇檔案 → 編碼選「UTF-8」

### CSV欄位說明

| 欄位 | 說明 | 範例 |
|------|------|------|
| search_keyword | 搜尋關鍵字 | AI自動化 |
| jobName | 職缺名稱 | AI流程自動化工程師 |
| custName | 公司名稱 | 規則王股份有限公司 |
| coIndustryDesc | 產業類別 | 餐飲業 |
| salaryLow | 最低薪資 | 40000 |
| salaryHigh | 最高薪資 | 60000 |
| salaryType | 薪資類型 | 月薪 |
| jobAddress | 工作地點 | 台北市信義區 |
| remoteWorkType | 遠端工作 | 部分遠端 |
| description | 工作內容 | 設計並建構內部自動化流程... |
| jobLink | 職缺連結 | https://www.104.com.tw/job/... |

---

## 進階設定

### 調整爬取速度

如果**被104封鎖**或**回應太慢**,修改 `.env`:

```bash
# 降低速度(更安全)
DOWNLOAD_DELAY=2.0              # 增加延遲
CONCURRENT_REQUESTS=8           # 減少同時請求

# 提高速度(風險較高)
DOWNLOAD_DELAY=0.25
CONCURRENT_REQUESTS=32
```

### 改變輸出格式

```bash
# 輸出JSON格式
OUTPUT_FORMAT="json"

# 輸出JSONLINES格式(每行一筆)
OUTPUT_FORMAT="jsonlines"
```

### 只爬取特定頁數

```bash
# 只爬第1頁(快速測試)
SCRAPY_PAGES_PER_KEYWORD=1

# 爬取更多頁
SCRAPY_PAGES_PER_KEYWORD=10
```

---

## 常見問題

### Q1: 執行時出現 "ModuleNotFoundError: No module named 'scrapy'"

**解決方法:**
```bash
pip install scrapy
# 或
pip3 install scrapy
```

### Q2: 執行時出現 "No module named 'dotenv'"

**解決方法:**
```bash
pip install python-dotenv
```

### Q3: 爬不到任何資料

**可能原因:**
1. 網路連線問題
2. 104網站改版
3. IP被暫時封鎖

**解決方法:**
```bash
# 增加延遲時間
DOWNLOAD_DELAY=2.0

# 等待30分鐘後再試
# 或使用VPN更換IP
```

### Q4: CSV檔案中文亂碼

**解決方法:**
1. 用記事本開啟 → 另存新檔 → 編碼選「UTF-8」
2. 或在Excel中:資料 → 從文字檔 → 選擇UTF-8編碼

### Q5: 想要更多職缺資料

**解決方法:**
```bash
# 方法1: 增加頁數
SCRAPY_PAGES_PER_KEYWORD=10

# 方法2: 加入更多關鍵字
SEARCH_KEYWORDS="AI自動化,AI轉型,數位轉型,流程自動化,RPA,AI工程師,機器學習,深度學習,數據分析"

# 方法3: 分次執行
# 先執行 AI 相關關鍵字
# 再執行其他關鍵字
```

### Q6: 執行速度太慢

**解決方法:**
```bash
# 減少頁數
SCRAPY_PAGES_PER_KEYWORD=3

# 減少關鍵字
SEARCH_KEYWORDS="AI自動化,RPA"

# 提高並發(風險:可能被封鎖)
CONCURRENT_REQUESTS=32
DOWNLOAD_DELAY=0.25
```

### Q7: 想要定期自動執行

**Windows:**
```bash
# 建立批次檔 run_scraper.bat
cd C:\your\project\path
scrapy crawl 104_ai_jobs
```
然後用「工作排程器」設定定期執行

**Mac/Linux:**
```bash
# 使用 crontab
crontab -e

# 每天早上9點執行
0 9 * * * cd /path/to/project && scrapy crawl 104_ai_jobs
```

---

## ⚠️ 重要提醒

### 法律與道德

- ✅ 個人求職使用:合理
- ✅ 學習研究用途:合理
- ❌ 大量商業轉售:不合理
- ❌ 過度頻繁爬取:不合理

### 建議做法

1. **設定適當延遲** (至少0.5秒)
2. **不要24小時連續爬取**
3. **尊重104的服務條款**
4. **僅爬取必要的資料量**

### 如果需要大量資料

請聯繫104官方API服務:
- 網站: https://www.104.com.tw
- 客服: 詢問企業API方案

---

## 🎉 完成!

現在你可以開始爬取AI相關職缺了!

如有問題,請檢查:
1. `.env` 設定是否正確
2. 網路連線是否正常
3. Python套件是否都已安裝

祝求職順利! 🚀
